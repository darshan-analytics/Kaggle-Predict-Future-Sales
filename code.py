# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BX_SpEsZeFkTgUvvJrN4pNlptlucSzK8
"""

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from itertools import product
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_blobs
from keras.layers import Dense
from keras.models import Sequential
from keras.optimizers import SGD
from keras.utils import to_categorical

import seaborn as sns
sns.set(style="whitegrid")

import math

import csv
import time
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_regression
import sklearn.metrics as metrics
import math

from sklearn.model_selection import cross_validate
from sklearn.model_selection import cross_val_score
from sklearn import metrics

#Loading the dataset into the code
df1 = pd.read_csv("items.csv")

'''Checking if there is any missing data into the different files and
printing the percentage of the missing values'''
for col in df1.columns:
    missing_data = np.mean(df1[col].isnull())
    print('{} - {}%'.format(col, round(missing_data*100)))




df2 = pd.read_csv("item_categories.csv")


for col in df2.columns:
    missing_data = np.mean(df2[col].isnull())
    print('{} - {}%'.format(col, round(missing_data*100)))

df3 = pd.read_csv("shops.csv")
df4 = pd.read_csv("sales_train.csv")
df5 = pd.read_csv("test.csv")

for col in df3.columns:
    missing_data = np.mean(df3[col].isnull())
    print('{} - {}%'.format(col, round(missing_data*100)))


for col in df4.columns:
    missing_data = np.mean(df4[col].isnull())
    print('{} - {}%'.format(col, round(missing_data*100)))


for col in df5.columns:
    missing_data = np.mean(df5[col].isnull())
    print('{} - {}%'.format(col, round(missing_data*100)))


#standardizing test dataset

standard_test = StandardScaler().fit_transform(df5)

#make 0 if item price or count is negative 
cal_med = df4['item_cnt_day']
#print(cal_med)
pd.options.mode.chained_assignment = None
cal_med[cal_med < 0] = 0

cal_remove = df4['item_price']
pd.options.mode.chained_assignment = None
cal_remove[cal_remove < 0] = 0


#print(cal_med)
#Performing analysis on the data
# drop duplicates
subset = ['date','date_block_num','shop_id','item_id','item_cnt_day']
print(df4.duplicated(subset=subset).value_counts())
df4.drop_duplicates(subset=subset, inplace=True)

check = df5['shop_id'].unique().any() not in df4['shop_id'].unique().tolist()
check_item = df5['item_id'].unique().any() not in df4['item_id'].unique().tolist()

print(check_item)
print(check)

pd.set_option('display.max_columns', 20)
pd.set_option('display.width', 2000)
print(df4.describe().T)

#checking unique values in the sales_train dataset
for col in df4.columns:
    print("Number of unique values of {} : {}".format(col, df4[col].nunique()))

#plotting the graph of shop_id vs item_id to get the plot


#grouping the month with the item count which they sell's 
plot_for_item_with_data = df4.groupby('date_block_num')['item_cnt_day'].sum()
plt.title('item_cnt along with the months')
plt.xlabel('month')
plt.ylabel('item_cnt')
plt.plot(plot_for_item_with_data)
plt.show()

#grouping the shop_id's for which the item count has been sold into the graph
plt.figure(figsize=(16,8))
shop_wise_count = df4.groupby(['shop_id'],as_index=False)['item_cnt_day'].sum()
plt.title('Sales by Shop')
plt.bar(range(len(shop_wise_count['item_cnt_day'])),list(shop_wise_count['item_cnt_day']), align='center')
plt.xticks(range(len(shop_wise_count['shop_id'])),list(shop_wise_count['shop_id'].unique()))
plt.ylabel('Sales')
plt.show()


#Doing data pre-processing from here on
#checking the name of the shop's are unique or not
df3['temp'] = df3['shop_name_translated'].str.split(' ').map(lambda x: x[1])
for i in df3['temp'].unique():
    if df3[df3['temp'] == i]['shop_id'].nunique() > 1:
        print(i)

#got the duplicate shop names with different ID's
print(df3[df3['temp'] == 'Ordzhonikidze,']['shop_name_translated'].unique())

print(df3[df3['shop_name_translated'].isin(['Yakutsk Ordzhonikidze, 56 francs','Yakutsk Ordzhonikidze, 56'])]['shop_id'].unique())

#print(df3[df3['temp'] == 'TRC']['shop_name_translated'].unique())
#print(df3[df3['temp'] == 'ТЦ']['shop_name_translated'].unique())

df3['city'] = df3['shop_name_translated'].str.split(' ').map(lambda x: x[0])
print(df3['city'].unique())

print('No. of shops is {} & no. of unique items is {}.'.format(df5['shop_id'].nunique(),df5['item_id'].nunique()))
assert len(df5) == df5['shop_id'].nunique() * df5['item_id'].nunique()

mat = []
col =['date_block_num','shop_id','item_id']
for i in range(34):
    df4_filtered = df4[df4['date_block_num'] == i]
    mat.append(np.array(list(product([i], df4_filtered['shop_id'].unique(), df4_filtered['item_id'].unique())), dtype='int16'))

#adding the year and month colun into the sales train data for the feature extraction
df4['Year'] = df4['date'].apply(lambda x:int(x.split(".")[2]))
df4['Month'] = df4['date'].apply(lambda x:int(x.split(".")[1]))
allFeatures = df4.drop(["date",  "item_price"], axis=1)
df = allFeatures.groupby(["Year", "Month","shop_id", "item_id"]).sum().reset_index()
df.rename(columns={'item_cnt_day':'item_cnt_month'},inplace = True)

#splitting the trainning and test data into X_train and X_test
X_train = df.query("not (Year == 2015 and Month == 10)").drop(["item_cnt_month"],axis=1)
y_train = df.query("not (Year == 2015 and Month == 10)")["item_cnt_month"]

X_test = df.query("Year == 2015 and Month == 10").drop(["item_cnt_month"],axis=1)
y_test = df.query("Year == 2015 and Month == 10")["item_cnt_month"]


#applying linear regression model on the train data and test data

regr_model = LinearRegression(normalize= True)

#fitting the model 
regr_model.fit(X_train,y_train)
print("Performing evaluation model on the dataset")

print("--------for train dataset------------")
def eval(model,X,y):
  y_train_hat = model.predict(X)
  mean_error = metrics.mean_absolute_error(y,y_train_hat)
  mean_sq = metrics.mean_squared_error(y,y_train_hat)

  print ("---------------------")
  print ("mean_error:                ", mean_error)
  print ("RMSE:               ", math.sqrt(mean_sq))
  print("----------------------")

print(eval(regr_model,X_train,y_train))

print("-----------for test set--------")

print(eval(regr_model,X_test, y_test))


#adding the year month and data_block for the test dataset
new = df5.copy()
new["Year"] = 2015
new["Month"] = 11
new["date_block_num"] = 33
x_cols = X_train.columns
new.drop("ID", axis = 1, inplace = True)
kaggle_test = new[x_cols]
new_y_hat = regr_model.predict(kaggle_test)


type(new_y_hat)
myneglist = [ -x for x in new_y_hat]
df_sub = pd.DataFrame(myneglist)
df_submission = pd.DataFrame(myneglist, columns=['item_cnt_month'])

#getting the file ready to submit into the .csv file
df_submission.to_csv("temp.csv",index=True,index_label="ID")
sampleSubmission = pd.read_csv("sample_submission.csv")

#finally saving the data into file along with it's ID and item count
df_submission['ID'] = sampleSubmission['ID']
final_submission = df_submission[['ID', 'item_cnt_month']]
final_submission.to_csv("prediction_set.csv", index=False)

from xgboost import XGBRegressor
from xgboost import plot_importance


#implementing and fitting the gradient descent on the dataset
model = XGBRegressor(
    max_depth=5,
    n_estimators=1000,
    min_child_weight=300, 
    colsample_bytree=0.8, 
    subsample=0.8, 
    eta=0.3,    
    seed=42)

model.fit(
    X_train, 
    y_train, 
    eval_metric="rmse", 
    eval_set=[(X_train, y_train), (X_test, y_test)], 
    verbose=True, 
    early_stopping_rounds = 10)

plot_for_item_with_data = df_submission.groupby('ID')['item_cnt_month'].sum()
plt.title('item_cnt along with the months')
plt.xlabel('item_cnt')
plt.ylabel('probability of each item')
plt.boxplot(plot_for_item_with_data)
plt.show()

plot_for_item_with_data = df_submission.groupby('ID')['item_cnt_month'].sum()
plt.title('item_cnt along with the months')
plt.xlabel('probability of each item')
plt.ylabel('item_cnt')
plt.hist(plot_for_item_with_data)
plt.show()